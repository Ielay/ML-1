{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as std_random\n",
    "from numpy import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv_data(file_name):\n",
    "    np_data_frame = pd.read_csv(file_name)\n",
    "    return np.array(np_data_frame.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y, y_pred):\n",
    "    return np.sum((y - y_pred)**2)/len(y_pred)\n",
    "\n",
    "\n",
    "def rmse(y, y_pred):\n",
    "    return np.sqrt(mse(y, y_pred))\n",
    "\n",
    "\n",
    "def r2(y, y_pred):\n",
    "    nom=np.sum((y - y_pred)**2)\n",
    "    denom=np.sum((y - np.mean(y, axis=0))**2)\n",
    "    return 1 - nom/denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_folds(data, k):\n",
    "    k_folds = np.array_split(data, k)\n",
    "    \n",
    "    x_data_folds = []\n",
    "    y_data_folds = []\n",
    "    for i in range(k):\n",
    "        x_data_folds.append(k_folds[i][:, :-1])\n",
    "        y_data_folds.append(k_folds[i][:, -1].reshape((-1, 1)))\n",
    "    \n",
    "    return x_data_folds, y_data_folds\n",
    "\n",
    "\n",
    "def concat(arr_to_concat):\n",
    "    concatenated = np.empty((0, arr_to_concat[0].shape[1]), int)\n",
    "    \n",
    "    for i in range (len(arr_to_concat)):\n",
    "        concatenated = np.concatenate((concatenated, arr_to_concat[i]))\n",
    "    \n",
    "    return concatenated\n",
    "\n",
    "\n",
    "def mean(data, _axis=0):\n",
    "    return np.mean(data, axis=_axis)\n",
    "\n",
    "\n",
    "def std(data, _axis=0):\n",
    "    return np.std(data, axis=_axis)\n",
    "\n",
    "\n",
    "def normalize_data(data):\n",
    "    _mean = mean(data)\n",
    "    _std = std(data)\n",
    "    # revome division by zero\n",
    "    np.place(_std, _std==0, 1)\n",
    "    \n",
    "    return (data - _mean) / _std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch(data, i, batch_size):\n",
    "    offset = i * batch_size\n",
    "    if offset + batch_size >= data.shape[0]:\n",
    "        return data[offset : data.shape[0], :]\n",
    "    else:\n",
    "        return data[offset : offset + batch_size, :]    \n",
    "\n",
    "\n",
    "def linear_regression(X, coeff, bias):\n",
    "    return X.dot(coeff) + bias\n",
    "\n",
    "\n",
    "def mini_batch_grad_desc(X, Y, learn_rate=0.001, epoch_numb=10, batch_size=800):\n",
    "    coeff = np.random.sample((X.shape[1], 1))\n",
    "    bias = 0\n",
    "    \n",
    "    rmse_arr = []\n",
    "    r2_arr = []\n",
    "    \n",
    "    for epoch in range(epoch_numb):\n",
    "        seed = std_random.randint(1, 10000)\n",
    "        \n",
    "        # shuffle before batch creating\n",
    "        std_random.Random(seed).shuffle(X)\n",
    "        std_random.Random(seed).shuffle(Y)\n",
    "        \n",
    "        number_of_batches = X.shape[0] // batch_size\n",
    "        if number_of_batches % batch_size != 0:\n",
    "            # 1 batch will have lower size if there are some train rows left\n",
    "            number_of_batches += 1\n",
    "        \n",
    "        for i in range(number_of_batches):\n",
    "            X_batch = create_batch(X, i, batch_size)\n",
    "            Y_batch = create_batch(Y, i, batch_size)\n",
    "            \n",
    "            #print(X_batch.shape)\n",
    "            #print(Y_batch.shape)\n",
    "            \n",
    "            batch_actual_size = X_batch.shape[0]\n",
    "            \n",
    "            # predict Y\n",
    "            Y_predicted = linear_regression(X_batch, coeff, bias)\n",
    "            error = Y_batch - Y_predicted\n",
    "            \n",
    "            # upd values using grad descent\n",
    "            coeff = coeff + learn_rate*2*((X_batch.T).dot(error)) / batch_actual_size\n",
    "            bias = bias + learn_rate*2*np.sum(error) / batch_actual_size\n",
    "            \n",
    "            #print(error)\n",
    "            \n",
    "            _rmse = rmse(Y_batch, Y_predicted)\n",
    "            _r2 = r2(Y_batch, Y_predicted)\n",
    "            \n",
    "            #print(\"rmse \" + str(epoch) + \"_\" + str(i) + \":\", _rmse)\n",
    "            #print(\"r2 \" + str(epoch) + \"_\" + str(i) + \":\", _r2)\n",
    "            \n",
    "            rmse_arr.append(_rmse)\n",
    "            r2_arr.append(_r2)\n",
    "        \n",
    "#     print(\"rmse:\", rmse_arr)\n",
    "#     print(\"r2:\", r2_arr)\n",
    "    return coeff, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.1836515217735737 35.838480663692 0.9191186454256817 4.177667226720612\n",
      "1: 0.16692428447171803 31.909535167837277 0.8272816740843763 4.285313334219525\n",
      "2: 0.2829324585805063 27.136391336340253 0.6752994486597668 5.167139991561769\n",
      "3: 0.298954232137738 28.423873147148708 0.3880165234148505 4.948019009233205\n",
      "4: 0.21035695760727613 32.334626814068436 0.7439804945723925 3.6211990282736095\n"
     ]
    }
   ],
   "source": [
    "train_data_f_name = \"./dataset/facebook_comment_volume.csv\"\n",
    "\n",
    "# get data for training\n",
    "train_data = get_csv_data(train_data_f_name)\n",
    "# print(np.shape(train_data))\n",
    "\n",
    "# shuffle it\n",
    "np.random.shuffle(train_data)\n",
    "\n",
    "# and make 5 folds\n",
    "x_folds, y_folds = split_to_folds(train_data, 5)\n",
    "\n",
    "# print(np.shape(x_train_folds[0]))\n",
    "# print(x_train_folds[0])\n",
    "# print(np.shape(y_train_folds[0]))\n",
    "# print(y_train_folds[0])\n",
    "\n",
    "coeff_arr = []\n",
    "bias_arr = []\n",
    "\n",
    "rmse_train_arr = []\n",
    "r2_train_arr = []\n",
    "rmse_test_arr = []\n",
    "r2_test_arr = []\n",
    "\n",
    "for i in range(len(x_folds)):\n",
    "    # make 1 test fold\n",
    "    x_test = x_folds[i]\n",
    "    y_test = y_folds[i]\n",
    "    \n",
    "    # and 4 train ones\n",
    "    x_train_folds = np.delete(x_folds, i)\n",
    "    y_train_folds = np.delete(y_folds, i)\n",
    "    \n",
    "    # concatenate 4 train folds into 1\n",
    "    x_train = concat(x_train_folds)\n",
    "    y_train = concat(y_train_folds)\n",
    "    \n",
    "#     print(np.shape(x_train))\n",
    "#     print(np.shape(y_train))\n",
    "    \n",
    "    # normalize train and test data\n",
    "    x_test = normalize_data(x_test)\n",
    "    x_train = normalize_data(x_train)\n",
    "    \n",
    "#     print(x_test)\n",
    "#     print(np.shape(x_test))\n",
    "#     print(x_test.shape[0])\n",
    "#     print(x_train)\n",
    "#     print(np.shape(x_train))\n",
    "\n",
    "    trained_coeff, trained_bias = mini_batch_grad_desc(x_train, y_train)\n",
    "    \n",
    "#     print(trained_coeff)\n",
    "#     print(trained_bias)\n",
    "    \n",
    "    coeff_arr.insert(i, trained_coeff)\n",
    "    bias_arr.insert(i, trained_bias)\n",
    "    \n",
    "    y_train_predicted = linear_regression(x_train, trained_coeff, trained_bias)\n",
    "    y_test_predicted = linear_regression(x_test, trained_coeff, trained_bias)\n",
    "    \n",
    "    # calculate metrics using trained coefficients for logging\n",
    "    rmse_train_arr.insert(i, rmse(y_train, y_train_predicted))\n",
    "    r2_train_arr.insert(i, r2(y_train, y_train_predicted))\n",
    "    rmse_test_arr.insert(i, rmse(y_test, y_test_predicted))\n",
    "    r2_test_arr.insert(i, r2(y_test, y_test_predicted))\n",
    "    \n",
    "    print(str(i) + \":\", r2_test_arr[i],rmse_test_arr[i],r2_train_arr[i],rmse_train_arr[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beautified output of calculated metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>E</th>\n",
       "      <th>SD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>test RMSE</th>\n",
       "      <td>35.838481</td>\n",
       "      <td>31.909535</td>\n",
       "      <td>27.136391</td>\n",
       "      <td>28.423873</td>\n",
       "      <td>32.334627</td>\n",
       "      <td>31.128581</td>\n",
       "      <td>3.082227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test R2</th>\n",
       "      <td>0.183652</td>\n",
       "      <td>0.166924</td>\n",
       "      <td>0.282932</td>\n",
       "      <td>0.298954</td>\n",
       "      <td>0.210357</td>\n",
       "      <td>0.228564</td>\n",
       "      <td>0.053026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train RMSE</th>\n",
       "      <td>4.177667</td>\n",
       "      <td>4.285313</td>\n",
       "      <td>5.167140</td>\n",
       "      <td>4.948019</td>\n",
       "      <td>3.621199</td>\n",
       "      <td>4.439868</td>\n",
       "      <td>0.556776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train R2</th>\n",
       "      <td>0.919119</td>\n",
       "      <td>0.827282</td>\n",
       "      <td>0.675299</td>\n",
       "      <td>0.388017</td>\n",
       "      <td>0.743980</td>\n",
       "      <td>0.710739</td>\n",
       "      <td>0.180840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bias</th>\n",
       "      <td>3.032512</td>\n",
       "      <td>3.618544</td>\n",
       "      <td>3.692879</td>\n",
       "      <td>3.348026</td>\n",
       "      <td>3.558849</td>\n",
       "      <td>3.450162</td>\n",
       "      <td>0.238302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.329670</td>\n",
       "      <td>0.376626</td>\n",
       "      <td>0.259049</td>\n",
       "      <td>-0.173634</td>\n",
       "      <td>0.190689</td>\n",
       "      <td>0.064612</td>\n",
       "      <td>0.269545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.589433</td>\n",
       "      <td>-0.879957</td>\n",
       "      <td>-0.420749</td>\n",
       "      <td>0.136868</td>\n",
       "      <td>-0.553072</td>\n",
       "      <td>-0.461269</td>\n",
       "      <td>0.334533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.161469</td>\n",
       "      <td>0.357536</td>\n",
       "      <td>0.486341</td>\n",
       "      <td>0.292659</td>\n",
       "      <td>0.649643</td>\n",
       "      <td>0.389530</td>\n",
       "      <td>0.167003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.272374</td>\n",
       "      <td>-0.100163</td>\n",
       "      <td>-0.159344</td>\n",
       "      <td>0.197867</td>\n",
       "      <td>0.237701</td>\n",
       "      <td>0.089687</td>\n",
       "      <td>0.181684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.157206</td>\n",
       "      <td>1.437103</td>\n",
       "      <td>1.415163</td>\n",
       "      <td>0.264105</td>\n",
       "      <td>0.143418</td>\n",
       "      <td>1.083399</td>\n",
       "      <td>0.767208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.606700</td>\n",
       "      <td>-0.227561</td>\n",
       "      <td>0.184264</td>\n",
       "      <td>-0.356745</td>\n",
       "      <td>0.205667</td>\n",
       "      <td>-0.160215</td>\n",
       "      <td>0.314656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.192630</td>\n",
       "      <td>1.117795</td>\n",
       "      <td>1.430124</td>\n",
       "      <td>0.451856</td>\n",
       "      <td>0.838814</td>\n",
       "      <td>1.006244</td>\n",
       "      <td>0.335332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.548096</td>\n",
       "      <td>0.870815</td>\n",
       "      <td>1.048519</td>\n",
       "      <td>0.326914</td>\n",
       "      <td>0.261817</td>\n",
       "      <td>0.811232</td>\n",
       "      <td>0.477335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.016577</td>\n",
       "      <td>0.319145</td>\n",
       "      <td>0.053218</td>\n",
       "      <td>0.396652</td>\n",
       "      <td>0.888933</td>\n",
       "      <td>0.334905</td>\n",
       "      <td>0.313588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.658958</td>\n",
       "      <td>1.557217</td>\n",
       "      <td>1.442301</td>\n",
       "      <td>0.344680</td>\n",
       "      <td>0.716752</td>\n",
       "      <td>1.543981</td>\n",
       "      <td>1.149458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.650160</td>\n",
       "      <td>-0.361598</td>\n",
       "      <td>-0.445190</td>\n",
       "      <td>-0.169373</td>\n",
       "      <td>-0.399747</td>\n",
       "      <td>-0.405214</td>\n",
       "      <td>0.154370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.301722</td>\n",
       "      <td>1.124249</td>\n",
       "      <td>0.487229</td>\n",
       "      <td>0.813087</td>\n",
       "      <td>0.944497</td>\n",
       "      <td>0.934157</td>\n",
       "      <td>0.277758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3.241475</td>\n",
       "      <td>2.018200</td>\n",
       "      <td>1.420970</td>\n",
       "      <td>1.396382</td>\n",
       "      <td>1.324194</td>\n",
       "      <td>1.880244</td>\n",
       "      <td>0.724738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.129774</td>\n",
       "      <td>0.293832</td>\n",
       "      <td>0.534075</td>\n",
       "      <td>0.423499</td>\n",
       "      <td>-0.088451</td>\n",
       "      <td>0.206636</td>\n",
       "      <td>0.269109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.279997</td>\n",
       "      <td>0.142549</td>\n",
       "      <td>0.482843</td>\n",
       "      <td>0.406745</td>\n",
       "      <td>0.614343</td>\n",
       "      <td>0.273297</td>\n",
       "      <td>0.316626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.206478</td>\n",
       "      <td>-0.206935</td>\n",
       "      <td>0.222468</td>\n",
       "      <td>-0.380331</td>\n",
       "      <td>0.103149</td>\n",
       "      <td>-0.093626</td>\n",
       "      <td>0.221996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.677867</td>\n",
       "      <td>0.847490</td>\n",
       "      <td>1.143729</td>\n",
       "      <td>1.049370</td>\n",
       "      <td>1.023912</td>\n",
       "      <td>1.148474</td>\n",
       "      <td>0.281499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.714053</td>\n",
       "      <td>2.782146</td>\n",
       "      <td>2.521988</td>\n",
       "      <td>0.685509</td>\n",
       "      <td>0.508802</td>\n",
       "      <td>1.842500</td>\n",
       "      <td>1.021922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.539577</td>\n",
       "      <td>0.034727</td>\n",
       "      <td>0.195618</td>\n",
       "      <td>0.403280</td>\n",
       "      <td>0.409561</td>\n",
       "      <td>0.316553</td>\n",
       "      <td>0.178865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.962248</td>\n",
       "      <td>1.254243</td>\n",
       "      <td>1.291864</td>\n",
       "      <td>0.644393</td>\n",
       "      <td>0.066031</td>\n",
       "      <td>1.043756</td>\n",
       "      <td>0.642639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.948355</td>\n",
       "      <td>-0.298580</td>\n",
       "      <td>-0.071833</td>\n",
       "      <td>0.146921</td>\n",
       "      <td>-0.135465</td>\n",
       "      <td>-0.261462</td>\n",
       "      <td>0.371971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.812544</td>\n",
       "      <td>0.517323</td>\n",
       "      <td>0.906503</td>\n",
       "      <td>0.611343</td>\n",
       "      <td>0.619610</td>\n",
       "      <td>0.693465</td>\n",
       "      <td>0.143380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.433474</td>\n",
       "      <td>1.069090</td>\n",
       "      <td>1.197540</td>\n",
       "      <td>0.792492</td>\n",
       "      <td>0.035124</td>\n",
       "      <td>0.905544</td>\n",
       "      <td>0.481899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.144183</td>\n",
       "      <td>0.216057</td>\n",
       "      <td>0.844234</td>\n",
       "      <td>0.628943</td>\n",
       "      <td>0.623981</td>\n",
       "      <td>0.433806</td>\n",
       "      <td>0.353322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.343267</td>\n",
       "      <td>1.309318</td>\n",
       "      <td>1.552682</td>\n",
       "      <td>1.075495</td>\n",
       "      <td>1.185891</td>\n",
       "      <td>1.293331</td>\n",
       "      <td>0.160629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.611721</td>\n",
       "      <td>-0.667037</td>\n",
       "      <td>-0.816777</td>\n",
       "      <td>-0.287331</td>\n",
       "      <td>-0.574367</td>\n",
       "      <td>-0.591447</td>\n",
       "      <td>0.173019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.742843</td>\n",
       "      <td>-0.094405</td>\n",
       "      <td>0.232046</td>\n",
       "      <td>-0.252457</td>\n",
       "      <td>0.772771</td>\n",
       "      <td>0.280160</td>\n",
       "      <td>0.420247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.745495</td>\n",
       "      <td>0.351062</td>\n",
       "      <td>0.355644</td>\n",
       "      <td>0.325375</td>\n",
       "      <td>-0.291797</td>\n",
       "      <td>0.297156</td>\n",
       "      <td>0.333165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.486363</td>\n",
       "      <td>0.123669</td>\n",
       "      <td>0.179663</td>\n",
       "      <td>-0.082324</td>\n",
       "      <td>-0.143775</td>\n",
       "      <td>-0.081826</td>\n",
       "      <td>0.235836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.711098</td>\n",
       "      <td>2.469390</td>\n",
       "      <td>1.522348</td>\n",
       "      <td>1.405581</td>\n",
       "      <td>1.439557</td>\n",
       "      <td>1.509595</td>\n",
       "      <td>0.561194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3.750070</td>\n",
       "      <td>5.223486</td>\n",
       "      <td>4.911160</td>\n",
       "      <td>4.180191</td>\n",
       "      <td>3.764519</td>\n",
       "      <td>4.365885</td>\n",
       "      <td>0.601358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-0.828757</td>\n",
       "      <td>0.460394</td>\n",
       "      <td>-0.053332</td>\n",
       "      <td>-0.470794</td>\n",
       "      <td>-0.353823</td>\n",
       "      <td>-0.249262</td>\n",
       "      <td>0.433087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.050938</td>\n",
       "      <td>2.566074</td>\n",
       "      <td>1.382658</td>\n",
       "      <td>1.520235</td>\n",
       "      <td>0.879488</td>\n",
       "      <td>1.479879</td>\n",
       "      <td>0.589128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>3.910397</td>\n",
       "      <td>3.642232</td>\n",
       "      <td>4.391671</td>\n",
       "      <td>3.667423</td>\n",
       "      <td>3.879352</td>\n",
       "      <td>3.898215</td>\n",
       "      <td>0.269368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-1.908907</td>\n",
       "      <td>-1.808309</td>\n",
       "      <td>-2.476102</td>\n",
       "      <td>-1.975392</td>\n",
       "      <td>-1.905708</td>\n",
       "      <td>-2.014884</td>\n",
       "      <td>0.236688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.354483</td>\n",
       "      <td>0.404074</td>\n",
       "      <td>-0.241041</td>\n",
       "      <td>0.147331</td>\n",
       "      <td>0.079469</td>\n",
       "      <td>0.148863</td>\n",
       "      <td>0.229893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.752063</td>\n",
       "      <td>1.263638</td>\n",
       "      <td>1.124315</td>\n",
       "      <td>1.320341</td>\n",
       "      <td>1.420859</td>\n",
       "      <td>1.176243</td>\n",
       "      <td>0.232751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.543167</td>\n",
       "      <td>0.438242</td>\n",
       "      <td>0.992320</td>\n",
       "      <td>0.942367</td>\n",
       "      <td>0.237912</td>\n",
       "      <td>0.630802</td>\n",
       "      <td>0.292195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1.231191</td>\n",
       "      <td>0.670358</td>\n",
       "      <td>0.745172</td>\n",
       "      <td>0.836683</td>\n",
       "      <td>0.705787</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.204354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.228516</td>\n",
       "      <td>0.382284</td>\n",
       "      <td>0.793275</td>\n",
       "      <td>0.664576</td>\n",
       "      <td>0.650480</td>\n",
       "      <td>0.743826</td>\n",
       "      <td>0.276795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.530711</td>\n",
       "      <td>0.628005</td>\n",
       "      <td>0.404873</td>\n",
       "      <td>0.329899</td>\n",
       "      <td>0.309783</td>\n",
       "      <td>0.440654</td>\n",
       "      <td>0.121560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.126072</td>\n",
       "      <td>0.839243</td>\n",
       "      <td>1.010273</td>\n",
       "      <td>0.558086</td>\n",
       "      <td>0.426099</td>\n",
       "      <td>0.591955</td>\n",
       "      <td>0.310433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.288588</td>\n",
       "      <td>0.458143</td>\n",
       "      <td>0.323490</td>\n",
       "      <td>0.173427</td>\n",
       "      <td>0.764248</td>\n",
       "      <td>0.401579</td>\n",
       "      <td>0.202804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.176368</td>\n",
       "      <td>0.801936</td>\n",
       "      <td>0.203475</td>\n",
       "      <td>0.772369</td>\n",
       "      <td>0.785981</td>\n",
       "      <td>0.548026</td>\n",
       "      <td>0.292666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.723320</td>\n",
       "      <td>0.368689</td>\n",
       "      <td>0.155467</td>\n",
       "      <td>0.266589</td>\n",
       "      <td>0.661648</td>\n",
       "      <td>0.435143</td>\n",
       "      <td>0.221538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.038057</td>\n",
       "      <td>0.140366</td>\n",
       "      <td>0.522700</td>\n",
       "      <td>0.493970</td>\n",
       "      <td>0.900403</td>\n",
       "      <td>0.419099</td>\n",
       "      <td>0.306880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.975649</td>\n",
       "      <td>0.417456</td>\n",
       "      <td>0.420765</td>\n",
       "      <td>0.627775</td>\n",
       "      <td>0.268884</td>\n",
       "      <td>0.542106</td>\n",
       "      <td>0.245035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>-0.062711</td>\n",
       "      <td>0.500599</td>\n",
       "      <td>0.704801</td>\n",
       "      <td>0.950052</td>\n",
       "      <td>0.732340</td>\n",
       "      <td>0.565016</td>\n",
       "      <td>0.344668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.468745</td>\n",
       "      <td>0.941395</td>\n",
       "      <td>1.300957</td>\n",
       "      <td>1.133529</td>\n",
       "      <td>0.389384</td>\n",
       "      <td>0.846802</td>\n",
       "      <td>0.360438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.555637</td>\n",
       "      <td>0.631510</td>\n",
       "      <td>0.579046</td>\n",
       "      <td>0.547685</td>\n",
       "      <td>0.646035</td>\n",
       "      <td>0.591983</td>\n",
       "      <td>0.039837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.835024</td>\n",
       "      <td>0.693850</td>\n",
       "      <td>0.302801</td>\n",
       "      <td>0.730353</td>\n",
       "      <td>0.093775</td>\n",
       "      <td>0.531161</td>\n",
       "      <td>0.283525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.173872</td>\n",
       "      <td>0.063155</td>\n",
       "      <td>0.142127</td>\n",
       "      <td>0.621020</td>\n",
       "      <td>-0.053203</td>\n",
       "      <td>0.189394</td>\n",
       "      <td>0.229594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>-0.191369</td>\n",
       "      <td>0.108460</td>\n",
       "      <td>0.401641</td>\n",
       "      <td>0.610834</td>\n",
       "      <td>0.632053</td>\n",
       "      <td>0.312324</td>\n",
       "      <td>0.314509</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    1          2          3          4          5          E  \\\n",
       "test RMSE   35.838481  31.909535  27.136391  28.423873  32.334627  31.128581   \n",
       "test R2      0.183652   0.166924   0.282932   0.298954   0.210357   0.228564   \n",
       "train RMSE   4.177667   4.285313   5.167140   4.948019   3.621199   4.439868   \n",
       "train R2     0.919119   0.827282   0.675299   0.388017   0.743980   0.710739   \n",
       "bias         3.032512   3.618544   3.692879   3.348026   3.558849   3.450162   \n",
       "0           -0.329670   0.376626   0.259049  -0.173634   0.190689   0.064612   \n",
       "1           -0.589433  -0.879957  -0.420749   0.136868  -0.553072  -0.461269   \n",
       "2            0.161469   0.357536   0.486341   0.292659   0.649643   0.389530   \n",
       "3            0.272374  -0.100163  -0.159344   0.197867   0.237701   0.089687   \n",
       "4            2.157206   1.437103   1.415163   0.264105   0.143418   1.083399   \n",
       "5           -0.606700  -0.227561   0.184264  -0.356745   0.205667  -0.160215   \n",
       "6            1.192630   1.117795   1.430124   0.451856   0.838814   1.006244   \n",
       "7            1.548096   0.870815   1.048519   0.326914   0.261817   0.811232   \n",
       "8            0.016577   0.319145   0.053218   0.396652   0.888933   0.334905   \n",
       "9            3.658958   1.557217   1.442301   0.344680   0.716752   1.543981   \n",
       "10          -0.650160  -0.361598  -0.445190  -0.169373  -0.399747  -0.405214   \n",
       "11           1.301722   1.124249   0.487229   0.813087   0.944497   0.934157   \n",
       "12           3.241475   2.018200   1.420970   1.396382   1.324194   1.880244   \n",
       "13          -0.129774   0.293832   0.534075   0.423499  -0.088451   0.206636   \n",
       "14          -0.279997   0.142549   0.482843   0.406745   0.614343   0.273297   \n",
       "15          -0.206478  -0.206935   0.222468  -0.380331   0.103149  -0.093626   \n",
       "16           1.677867   0.847490   1.143729   1.049370   1.023912   1.148474   \n",
       "17           2.714053   2.782146   2.521988   0.685509   0.508802   1.842500   \n",
       "18           0.539577   0.034727   0.195618   0.403280   0.409561   0.316553   \n",
       "19           1.962248   1.254243   1.291864   0.644393   0.066031   1.043756   \n",
       "20          -0.948355  -0.298580  -0.071833   0.146921  -0.135465  -0.261462   \n",
       "21           0.812544   0.517323   0.906503   0.611343   0.619610   0.693465   \n",
       "22           1.433474   1.069090   1.197540   0.792492   0.035124   0.905544   \n",
       "23          -0.144183   0.216057   0.844234   0.628943   0.623981   0.433806   \n",
       "24           1.343267   1.309318   1.552682   1.075495   1.185891   1.293331   \n",
       "25          -0.611721  -0.667037  -0.816777  -0.287331  -0.574367  -0.591447   \n",
       "26           0.742843  -0.094405   0.232046  -0.252457   0.772771   0.280160   \n",
       "27           0.745495   0.351062   0.355644   0.325375  -0.291797   0.297156   \n",
       "28          -0.486363   0.123669   0.179663  -0.082324  -0.143775  -0.081826   \n",
       "29           0.711098   2.469390   1.522348   1.405581   1.439557   1.509595   \n",
       "30           3.750070   5.223486   4.911160   4.180191   3.764519   4.365885   \n",
       "31          -0.828757   0.460394  -0.053332  -0.470794  -0.353823  -0.249262   \n",
       "32           1.050938   2.566074   1.382658   1.520235   0.879488   1.479879   \n",
       "33           3.910397   3.642232   4.391671   3.667423   3.879352   3.898215   \n",
       "34          -1.908907  -1.808309  -2.476102  -1.975392  -1.905708  -2.014884   \n",
       "35           0.354483   0.404074  -0.241041   0.147331   0.079469   0.148863   \n",
       "36           0.752063   1.263638   1.124315   1.320341   1.420859   1.176243   \n",
       "37           0.543167   0.438242   0.992320   0.942367   0.237912   0.630802   \n",
       "38           1.231191   0.670358   0.745172   0.836683   0.705787   0.837838   \n",
       "39           1.228516   0.382284   0.793275   0.664576   0.650480   0.743826   \n",
       "40           0.530711   0.628005   0.404873   0.329899   0.309783   0.440654   \n",
       "41           0.126072   0.839243   1.010273   0.558086   0.426099   0.591955   \n",
       "42           0.288588   0.458143   0.323490   0.173427   0.764248   0.401579   \n",
       "43           0.176368   0.801936   0.203475   0.772369   0.785981   0.548026   \n",
       "44           0.723320   0.368689   0.155467   0.266589   0.661648   0.435143   \n",
       "45           0.038057   0.140366   0.522700   0.493970   0.900403   0.419099   \n",
       "46           0.975649   0.417456   0.420765   0.627775   0.268884   0.542106   \n",
       "47          -0.062711   0.500599   0.704801   0.950052   0.732340   0.565016   \n",
       "48           0.468745   0.941395   1.300957   1.133529   0.389384   0.846802   \n",
       "49           0.555637   0.631510   0.579046   0.547685   0.646035   0.591983   \n",
       "50           0.835024   0.693850   0.302801   0.730353   0.093775   0.531161   \n",
       "51           0.173872   0.063155   0.142127   0.621020  -0.053203   0.189394   \n",
       "52          -0.191369   0.108460   0.401641   0.610834   0.632053   0.312324   \n",
       "\n",
       "                  SD  \n",
       "test RMSE   3.082227  \n",
       "test R2     0.053026  \n",
       "train RMSE  0.556776  \n",
       "train R2    0.180840  \n",
       "bias        0.238302  \n",
       "0           0.269545  \n",
       "1           0.334533  \n",
       "2           0.167003  \n",
       "3           0.181684  \n",
       "4           0.767208  \n",
       "5           0.314656  \n",
       "6           0.335332  \n",
       "7           0.477335  \n",
       "8           0.313588  \n",
       "9           1.149458  \n",
       "10          0.154370  \n",
       "11          0.277758  \n",
       "12          0.724738  \n",
       "13          0.269109  \n",
       "14          0.316626  \n",
       "15          0.221996  \n",
       "16          0.281499  \n",
       "17          1.021922  \n",
       "18          0.178865  \n",
       "19          0.642639  \n",
       "20          0.371971  \n",
       "21          0.143380  \n",
       "22          0.481899  \n",
       "23          0.353322  \n",
       "24          0.160629  \n",
       "25          0.173019  \n",
       "26          0.420247  \n",
       "27          0.333165  \n",
       "28          0.235836  \n",
       "29          0.561194  \n",
       "30          0.601358  \n",
       "31          0.433087  \n",
       "32          0.589128  \n",
       "33          0.269368  \n",
       "34          0.236688  \n",
       "35          0.229893  \n",
       "36          0.232751  \n",
       "37          0.292195  \n",
       "38          0.204354  \n",
       "39          0.276795  \n",
       "40          0.121560  \n",
       "41          0.310433  \n",
       "42          0.202804  \n",
       "43          0.292666  \n",
       "44          0.221538  \n",
       "45          0.306880  \n",
       "46          0.245035  \n",
       "47          0.344668  \n",
       "48          0.360438  \n",
       "49          0.039837  \n",
       "50          0.283525  \n",
       "51          0.229594  \n",
       "52          0.314509  "
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame_data = {}\n",
    "for i in range(5):\n",
    "    frame_data[str(i + 1)] = [rmse_test_arr[i], r2_test_arr[i], rmse_train_arr[i], r2_train_arr[i], bias_arr[i]]\n",
    "\n",
    "# print(data)\n",
    "\n",
    "frame_data['E'] = [np.mean(rmse_test_arr), np.mean(r2_test_arr), np.mean(rmse_train_arr), np.mean(r2_train_arr), np.mean(bias_arr)]\n",
    "frame_data['SD'] = [np.std(rmse_test_arr),np.std(r2_test_arr),np.std(rmse_train_arr),np.std(r2_train_arr), np.std(bias_arr)]\n",
    " \n",
    "df1 = pd.DataFrame(frame_data, index =['test RMSE', 'test R2','train RMSE','train R2', 'bias']) \n",
    "df2 = pd.DataFrame(\n",
    "    np.concatenate(\n",
    "        (np.hstack(coeff_arr), \n",
    "         np.mean(coeff_arr, axis=0),\n",
    "         np.std(coeff_arr, axis=0)),\n",
    "        axis=1),\n",
    "    columns=['1', '2', '3','4','5','E','SD'])\n",
    "\n",
    "df = pd.concat([df1, df2], axis=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
