{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv_data(file_name):\n",
    "    np_data_frame = pd.read_csv(file_name)\n",
    "    return np.array(np_data_frame.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y, y_pred):\n",
    "    return np.sum((y - y_pred)**2)/len(y_pred)\n",
    "\n",
    "\n",
    "def rmse(y, y_pred):\n",
    "    return np.sqrt(mse(y, y_pred))\n",
    "\n",
    "\n",
    "def r2(y, y_pred):\n",
    "    nom=np.sum((y - y_pred)**2)\n",
    "    denom=np.sum((y - np.mean(y, axis=0))**2)\n",
    "    return 1 - nom/denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_folds(data, k):\n",
    "    k_folds = np.array_split(data, k)\n",
    "    \n",
    "    x_data_folds = []\n",
    "    y_data_folds = []\n",
    "    for i in range(k):\n",
    "        x_data_folds.append(k_folds[i][:, :-1])\n",
    "        y_data_folds.append(k_folds[i][:, -1].reshape((-1, 1)))\n",
    "    \n",
    "    return x_data_folds, y_data_folds\n",
    "\n",
    "\n",
    "def concat(arr_to_concat):\n",
    "    concatenated = np.empty((0, arr_to_concat[0].shape[1]), int)\n",
    "    \n",
    "    for i in range (len(arr_to_concat)):\n",
    "        concatenated = np.concatenate((concatenated, arr_to_concat[i]))\n",
    "    \n",
    "    return concatenated\n",
    "\n",
    "\n",
    "def mean(data, _axis=0):\n",
    "    return np.mean(data, axis=_axis)\n",
    "\n",
    "\n",
    "def std(data, _axis=0):\n",
    "    return np.std(data, axis=_axis)\n",
    "\n",
    "\n",
    "def normalize_data(data):\n",
    "    _mean = mean(data)\n",
    "    _std = std(data)\n",
    "    # revome division by zero\n",
    "    np.place(_std, _std==0, 1)\n",
    "    \n",
    "    return (data - _mean) / _std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch(data, i, batch_size):\n",
    "    offset = i * batch_size\n",
    "    if offset + batch_size >= data.shape[0]:\n",
    "        return data[offset : data.shape[0], :]\n",
    "    else:\n",
    "        return data[offset : offset + batch_size, :]    \n",
    "\n",
    "\n",
    "def linear_regression(X, coeff, bias):\n",
    "    return X.dot(coeff) + bias\n",
    "\n",
    "\n",
    "def mini_batch_grad_desc(X, Y, learn_rate=0.001, epoch_numb=10, batch_size=800):\n",
    "    coeff = np.random.sample((X.shape[1], 1))\n",
    "    bias = 0\n",
    "    \n",
    "    rmse_arr = []\n",
    "    r2_arr = []\n",
    "    \n",
    "    for epoch in range(epoch_numb):\n",
    "        seed = np.random.randint(0, 10000)\n",
    "        \n",
    "        # shuffle data before batch creating\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(X)\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(Y)\n",
    "        \n",
    "        number_of_batches = X.shape[0] // batch_size\n",
    "        if number_of_batches % batch_size != 0:\n",
    "            # 1 batch will have lower size if there are some train rows left\n",
    "            number_of_batches += 1\n",
    "        \n",
    "        for i in range(number_of_batches):\n",
    "            X_batch = create_batch(X, i, batch_size)\n",
    "            Y_batch = create_batch(Y, i, batch_size)\n",
    "            \n",
    "            #print(X_batch.shape)\n",
    "            #print(Y_batch.shape)\n",
    "            \n",
    "            batch_actual_size = X_batch.shape[0]\n",
    "            \n",
    "            # predict Y\n",
    "            Y_predicted = linear_regression(X_batch, coeff, bias)\n",
    "            error = Y_batch - Y_predicted\n",
    "            \n",
    "            # upd values using grad descent\n",
    "            coeff = coeff + learn_rate*2*((X_batch.T).dot(error)) / batch_actual_size\n",
    "            bias = bias + learn_rate*2*np.sum(error) / batch_actual_size\n",
    "            \n",
    "            #print(error)\n",
    "            \n",
    "            _rmse = rmse(Y_batch, Y_predicted)\n",
    "            _r2 = r2(Y_batch, Y_predicted)\n",
    "            \n",
    "            #print(\"rmse \" + str(epoch) + \"_\" + str(i) + \":\", _rmse)\n",
    "            #print(\"r2 \" + str(epoch) + \"_\" + str(i) + \":\", _r2)\n",
    "            \n",
    "            rmse_arr.append(_rmse)\n",
    "            r2_arr.append(_r2)\n",
    "        \n",
    "#     print(\"rmse:\", rmse_arr)\n",
    "#     print(\"r2:\", r2_arr)\n",
    "    return coeff, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.23302655715568388 31.95928776319663 0.31200124951976993 29.23013567662983\n",
      "1: 0.23104437443289783 25.129474371178603 0.3071534010301822 30.801062730241537\n",
      "2: 0.31045210058654205 31.050240989346964 0.2906362933777846 29.481633410758146\n",
      "3: 0.3080280341696481 33.630573479710755 0.2861701892274785 28.851709899756163\n",
      "4: 0.34652567735258055 26.963717511256615 0.28017067437738685 30.551389960444858\n"
     ]
    }
   ],
   "source": [
    "train_data_f_name = \"./dataset/facebook_comment_volume.csv\"\n",
    "\n",
    "# get data for training\n",
    "train_data = get_csv_data(train_data_f_name)\n",
    "# print(np.shape(train_data))\n",
    "\n",
    "# shuffle it\n",
    "np.random.shuffle(train_data)\n",
    "\n",
    "# and make 5 folds\n",
    "x_folds, y_folds = split_to_folds(train_data, 5)\n",
    "\n",
    "# print(np.shape(x_train_folds[0]))\n",
    "# print(x_train_folds[0])\n",
    "# print(np.shape(y_train_folds[0]))\n",
    "# print(y_train_folds[0])\n",
    "\n",
    "coeff_arr = []\n",
    "bias_arr = []\n",
    "\n",
    "rmse_train_arr = []\n",
    "r2_train_arr = []\n",
    "rmse_test_arr = []\n",
    "r2_test_arr = []\n",
    "\n",
    "for i in range(len(x_folds)):\n",
    "    # make 1 test fold\n",
    "    x_test = x_folds[i]\n",
    "    y_test = y_folds[i]\n",
    "    \n",
    "    # and 4 train ones\n",
    "    x_train_folds = np.delete(x_folds, i)\n",
    "    y_train_folds = np.delete(y_folds, i)\n",
    "    \n",
    "    # concatenate 4 train folds into 1\n",
    "    x_train = concat(x_train_folds)\n",
    "    y_train = concat(y_train_folds)\n",
    "    \n",
    "#     print(np.shape(x_train))\n",
    "#     print(np.shape(y_train))\n",
    "    \n",
    "    # normalize train and test data\n",
    "    x_test = normalize_data(x_test)\n",
    "    x_train = normalize_data(x_train)\n",
    "    \n",
    "#     print(x_test)\n",
    "#     print(np.shape(x_test))\n",
    "#     print(x_test.shape[0])\n",
    "#     print(x_train)\n",
    "#     print(np.shape(x_train))\n",
    "\n",
    "    trained_coeff, trained_bias = mini_batch_grad_desc(x_train, y_train)\n",
    "    \n",
    "#     print(trained_coeff)\n",
    "#     print(trained_bias)\n",
    "    \n",
    "    coeff_arr.insert(i, trained_coeff)\n",
    "    bias_arr.insert(i, trained_bias)\n",
    "    \n",
    "    y_train_predicted = linear_regression(x_train, trained_coeff, trained_bias)\n",
    "    y_test_predicted = linear_regression(x_test, trained_coeff, trained_bias)\n",
    "    \n",
    "    # calculate metrics using trained coefficients for logging\n",
    "    rmse_train_arr.insert(i, rmse(y_train, y_train_predicted))\n",
    "    r2_train_arr.insert(i, r2(y_train, y_train_predicted))\n",
    "    rmse_test_arr.insert(i, rmse(y_test, y_test_predicted))\n",
    "    r2_test_arr.insert(i, r2(y_test, y_test_predicted))\n",
    "    \n",
    "    print(str(i) + \":\", r2_test_arr[i],rmse_test_arr[i],r2_train_arr[i],rmse_train_arr[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beautified output of calculated metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>E</th>\n",
       "      <th>SD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>test RMSE</th>\n",
       "      <td>31.959288</td>\n",
       "      <td>25.129474</td>\n",
       "      <td>31.050241</td>\n",
       "      <td>33.630573</td>\n",
       "      <td>26.963718</td>\n",
       "      <td>29.746659</td>\n",
       "      <td>3.185687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test R2</th>\n",
       "      <td>0.233027</td>\n",
       "      <td>0.231044</td>\n",
       "      <td>0.310452</td>\n",
       "      <td>0.308028</td>\n",
       "      <td>0.346526</td>\n",
       "      <td>0.285815</td>\n",
       "      <td>0.045984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train RMSE</th>\n",
       "      <td>29.230136</td>\n",
       "      <td>30.801063</td>\n",
       "      <td>29.481633</td>\n",
       "      <td>28.851710</td>\n",
       "      <td>30.551390</td>\n",
       "      <td>29.783186</td>\n",
       "      <td>0.760349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train R2</th>\n",
       "      <td>0.312001</td>\n",
       "      <td>0.307153</td>\n",
       "      <td>0.290636</td>\n",
       "      <td>0.286170</td>\n",
       "      <td>0.280171</td>\n",
       "      <td>0.295226</td>\n",
       "      <td>0.012275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bias</th>\n",
       "      <td>4.053833</td>\n",
       "      <td>4.222569</td>\n",
       "      <td>4.090705</td>\n",
       "      <td>4.029523</td>\n",
       "      <td>4.103351</td>\n",
       "      <td>4.099996</td>\n",
       "      <td>0.066662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.185145</td>\n",
       "      <td>0.206990</td>\n",
       "      <td>0.150106</td>\n",
       "      <td>-0.287481</td>\n",
       "      <td>0.148342</td>\n",
       "      <td>0.006562</td>\n",
       "      <td>0.202035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.535028</td>\n",
       "      <td>-0.244802</td>\n",
       "      <td>-0.281763</td>\n",
       "      <td>0.002218</td>\n",
       "      <td>-0.156190</td>\n",
       "      <td>-0.243113</td>\n",
       "      <td>0.175758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.039962</td>\n",
       "      <td>-0.331414</td>\n",
       "      <td>-0.340516</td>\n",
       "      <td>0.029870</td>\n",
       "      <td>-0.029738</td>\n",
       "      <td>-0.142352</td>\n",
       "      <td>0.159900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.130316</td>\n",
       "      <td>0.525466</td>\n",
       "      <td>0.424846</td>\n",
       "      <td>0.214428</td>\n",
       "      <td>0.340961</td>\n",
       "      <td>0.327203</td>\n",
       "      <td>0.141785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.237178</td>\n",
       "      <td>-0.184321</td>\n",
       "      <td>0.001074</td>\n",
       "      <td>-0.095950</td>\n",
       "      <td>0.246647</td>\n",
       "      <td>0.040925</td>\n",
       "      <td>0.174296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.265198</td>\n",
       "      <td>-0.130762</td>\n",
       "      <td>-0.215418</td>\n",
       "      <td>0.198282</td>\n",
       "      <td>-0.231907</td>\n",
       "      <td>-0.022922</td>\n",
       "      <td>0.211803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.640736</td>\n",
       "      <td>0.628721</td>\n",
       "      <td>0.240620</td>\n",
       "      <td>0.565513</td>\n",
       "      <td>0.948657</td>\n",
       "      <td>0.604849</td>\n",
       "      <td>0.225509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.574431</td>\n",
       "      <td>0.301874</td>\n",
       "      <td>0.606753</td>\n",
       "      <td>0.575068</td>\n",
       "      <td>0.627206</td>\n",
       "      <td>0.537066</td>\n",
       "      <td>0.119279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.573064</td>\n",
       "      <td>0.059669</td>\n",
       "      <td>0.611451</td>\n",
       "      <td>0.858294</td>\n",
       "      <td>0.815341</td>\n",
       "      <td>0.583564</td>\n",
       "      <td>0.284447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.457719</td>\n",
       "      <td>0.369495</td>\n",
       "      <td>0.286830</td>\n",
       "      <td>0.056105</td>\n",
       "      <td>-0.011567</td>\n",
       "      <td>0.048629</td>\n",
       "      <td>0.289746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.150073</td>\n",
       "      <td>-0.401864</td>\n",
       "      <td>-0.392294</td>\n",
       "      <td>-0.048610</td>\n",
       "      <td>-0.126313</td>\n",
       "      <td>-0.163802</td>\n",
       "      <td>0.210748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.348487</td>\n",
       "      <td>0.611964</td>\n",
       "      <td>1.130787</td>\n",
       "      <td>0.314873</td>\n",
       "      <td>0.625295</td>\n",
       "      <td>0.606281</td>\n",
       "      <td>0.292191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.967819</td>\n",
       "      <td>1.536527</td>\n",
       "      <td>0.708636</td>\n",
       "      <td>0.606355</td>\n",
       "      <td>0.747657</td>\n",
       "      <td>0.913399</td>\n",
       "      <td>0.333144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.803003</td>\n",
       "      <td>0.248467</td>\n",
       "      <td>0.026466</td>\n",
       "      <td>0.233662</td>\n",
       "      <td>0.506142</td>\n",
       "      <td>0.363548</td>\n",
       "      <td>0.267280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.112968</td>\n",
       "      <td>-0.156520</td>\n",
       "      <td>-0.001331</td>\n",
       "      <td>-0.147935</td>\n",
       "      <td>-0.033909</td>\n",
       "      <td>-0.045345</td>\n",
       "      <td>0.100016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.230173</td>\n",
       "      <td>0.186754</td>\n",
       "      <td>0.726976</td>\n",
       "      <td>0.612256</td>\n",
       "      <td>0.100950</td>\n",
       "      <td>0.371421</td>\n",
       "      <td>0.249651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.637532</td>\n",
       "      <td>0.773758</td>\n",
       "      <td>1.242153</td>\n",
       "      <td>0.277992</td>\n",
       "      <td>0.939806</td>\n",
       "      <td>0.774249</td>\n",
       "      <td>0.319783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.294922</td>\n",
       "      <td>1.080646</td>\n",
       "      <td>1.155135</td>\n",
       "      <td>0.905404</td>\n",
       "      <td>1.201230</td>\n",
       "      <td>1.127468</td>\n",
       "      <td>0.130960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.256362</td>\n",
       "      <td>0.406979</td>\n",
       "      <td>0.286445</td>\n",
       "      <td>0.153531</td>\n",
       "      <td>0.783381</td>\n",
       "      <td>0.377340</td>\n",
       "      <td>0.218511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.767448</td>\n",
       "      <td>-0.107261</td>\n",
       "      <td>-0.396583</td>\n",
       "      <td>0.417241</td>\n",
       "      <td>-0.325034</td>\n",
       "      <td>0.071162</td>\n",
       "      <td>0.449928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.152091</td>\n",
       "      <td>0.337135</td>\n",
       "      <td>-0.030420</td>\n",
       "      <td>0.178414</td>\n",
       "      <td>0.005823</td>\n",
       "      <td>0.128608</td>\n",
       "      <td>0.131819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.185779</td>\n",
       "      <td>0.766288</td>\n",
       "      <td>0.312692</td>\n",
       "      <td>0.828143</td>\n",
       "      <td>0.179224</td>\n",
       "      <td>0.454425</td>\n",
       "      <td>0.284576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.650481</td>\n",
       "      <td>0.981868</td>\n",
       "      <td>0.483993</td>\n",
       "      <td>0.425390</td>\n",
       "      <td>0.913073</td>\n",
       "      <td>0.690961</td>\n",
       "      <td>0.223142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.048967</td>\n",
       "      <td>0.600821</td>\n",
       "      <td>0.789287</td>\n",
       "      <td>0.608161</td>\n",
       "      <td>0.143100</td>\n",
       "      <td>0.438067</td>\n",
       "      <td>0.288853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.872690</td>\n",
       "      <td>0.481308</td>\n",
       "      <td>1.162291</td>\n",
       "      <td>0.947374</td>\n",
       "      <td>0.921261</td>\n",
       "      <td>0.876985</td>\n",
       "      <td>0.221311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.147655</td>\n",
       "      <td>-0.195275</td>\n",
       "      <td>-0.050447</td>\n",
       "      <td>-0.321924</td>\n",
       "      <td>-0.188841</td>\n",
       "      <td>-0.180828</td>\n",
       "      <td>0.087499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.060411</td>\n",
       "      <td>0.955831</td>\n",
       "      <td>0.753520</td>\n",
       "      <td>0.572352</td>\n",
       "      <td>0.748701</td>\n",
       "      <td>0.618163</td>\n",
       "      <td>0.304160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.209860</td>\n",
       "      <td>0.201829</td>\n",
       "      <td>0.033375</td>\n",
       "      <td>-0.132778</td>\n",
       "      <td>0.292927</td>\n",
       "      <td>0.121043</td>\n",
       "      <td>0.152361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.411891</td>\n",
       "      <td>0.827022</td>\n",
       "      <td>0.755576</td>\n",
       "      <td>0.641470</td>\n",
       "      <td>0.756706</td>\n",
       "      <td>0.678533</td>\n",
       "      <td>0.145991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2.281307</td>\n",
       "      <td>1.696922</td>\n",
       "      <td>1.345679</td>\n",
       "      <td>1.719093</td>\n",
       "      <td>1.904426</td>\n",
       "      <td>1.789485</td>\n",
       "      <td>0.305151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>5.792997</td>\n",
       "      <td>6.265100</td>\n",
       "      <td>5.031915</td>\n",
       "      <td>5.619893</td>\n",
       "      <td>5.585135</td>\n",
       "      <td>5.659008</td>\n",
       "      <td>0.396367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-0.586966</td>\n",
       "      <td>-0.430139</td>\n",
       "      <td>-0.554167</td>\n",
       "      <td>-0.302275</td>\n",
       "      <td>-0.590787</td>\n",
       "      <td>-0.492867</td>\n",
       "      <td>0.111769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.602976</td>\n",
       "      <td>1.672224</td>\n",
       "      <td>1.559883</td>\n",
       "      <td>1.661351</td>\n",
       "      <td>1.407887</td>\n",
       "      <td>1.580864</td>\n",
       "      <td>0.095577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>5.690387</td>\n",
       "      <td>6.109222</td>\n",
       "      <td>5.297674</td>\n",
       "      <td>5.159223</td>\n",
       "      <td>5.417433</td>\n",
       "      <td>5.534788</td>\n",
       "      <td>0.336223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-2.979768</td>\n",
       "      <td>-3.045430</td>\n",
       "      <td>-3.017600</td>\n",
       "      <td>-3.262161</td>\n",
       "      <td>-3.175854</td>\n",
       "      <td>-3.096163</td>\n",
       "      <td>0.106018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.162338</td>\n",
       "      <td>0.270715</td>\n",
       "      <td>0.176411</td>\n",
       "      <td>0.140795</td>\n",
       "      <td>0.296265</td>\n",
       "      <td>0.209305</td>\n",
       "      <td>0.062153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1.447125</td>\n",
       "      <td>1.435239</td>\n",
       "      <td>2.849905</td>\n",
       "      <td>1.375271</td>\n",
       "      <td>0.981331</td>\n",
       "      <td>1.617774</td>\n",
       "      <td>0.639446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.262264</td>\n",
       "      <td>0.172443</td>\n",
       "      <td>0.121357</td>\n",
       "      <td>0.588367</td>\n",
       "      <td>0.374004</td>\n",
       "      <td>0.303687</td>\n",
       "      <td>0.166236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.771000</td>\n",
       "      <td>0.999330</td>\n",
       "      <td>1.026439</td>\n",
       "      <td>0.877061</td>\n",
       "      <td>0.797910</td>\n",
       "      <td>0.894348</td>\n",
       "      <td>0.103231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.424092</td>\n",
       "      <td>0.223100</td>\n",
       "      <td>0.548597</td>\n",
       "      <td>0.194176</td>\n",
       "      <td>0.449813</td>\n",
       "      <td>0.367956</td>\n",
       "      <td>0.136869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.192744</td>\n",
       "      <td>0.424386</td>\n",
       "      <td>0.636028</td>\n",
       "      <td>0.387287</td>\n",
       "      <td>0.709674</td>\n",
       "      <td>0.470024</td>\n",
       "      <td>0.184825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.377326</td>\n",
       "      <td>0.418348</td>\n",
       "      <td>0.804390</td>\n",
       "      <td>0.471760</td>\n",
       "      <td>0.713274</td>\n",
       "      <td>0.557020</td>\n",
       "      <td>0.169939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.866423</td>\n",
       "      <td>0.640267</td>\n",
       "      <td>1.017143</td>\n",
       "      <td>0.715932</td>\n",
       "      <td>0.885382</td>\n",
       "      <td>0.825030</td>\n",
       "      <td>0.132893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.675915</td>\n",
       "      <td>0.333509</td>\n",
       "      <td>0.817166</td>\n",
       "      <td>0.546092</td>\n",
       "      <td>0.625919</td>\n",
       "      <td>0.599720</td>\n",
       "      <td>0.159699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.479153</td>\n",
       "      <td>0.298036</td>\n",
       "      <td>0.849087</td>\n",
       "      <td>0.520984</td>\n",
       "      <td>0.691269</td>\n",
       "      <td>0.567706</td>\n",
       "      <td>0.188250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.318169</td>\n",
       "      <td>0.069147</td>\n",
       "      <td>0.606889</td>\n",
       "      <td>0.219041</td>\n",
       "      <td>0.218251</td>\n",
       "      <td>0.286300</td>\n",
       "      <td>0.178943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.418424</td>\n",
       "      <td>0.502369</td>\n",
       "      <td>0.239357</td>\n",
       "      <td>0.295634</td>\n",
       "      <td>0.224959</td>\n",
       "      <td>0.336148</td>\n",
       "      <td>0.107491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.708145</td>\n",
       "      <td>0.781429</td>\n",
       "      <td>0.452567</td>\n",
       "      <td>0.641150</td>\n",
       "      <td>0.416624</td>\n",
       "      <td>0.599983</td>\n",
       "      <td>0.142596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.484376</td>\n",
       "      <td>0.427635</td>\n",
       "      <td>0.647901</td>\n",
       "      <td>0.523983</td>\n",
       "      <td>0.187129</td>\n",
       "      <td>0.454205</td>\n",
       "      <td>0.151877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.573616</td>\n",
       "      <td>0.682171</td>\n",
       "      <td>0.474228</td>\n",
       "      <td>0.479099</td>\n",
       "      <td>0.549551</td>\n",
       "      <td>0.551733</td>\n",
       "      <td>0.075869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.513133</td>\n",
       "      <td>0.419905</td>\n",
       "      <td>0.572777</td>\n",
       "      <td>0.447975</td>\n",
       "      <td>0.112866</td>\n",
       "      <td>0.413331</td>\n",
       "      <td>0.159315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.400539</td>\n",
       "      <td>0.609058</td>\n",
       "      <td>0.640111</td>\n",
       "      <td>0.315815</td>\n",
       "      <td>0.400344</td>\n",
       "      <td>0.473173</td>\n",
       "      <td>0.127808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.207587</td>\n",
       "      <td>0.668357</td>\n",
       "      <td>0.360561</td>\n",
       "      <td>0.493982</td>\n",
       "      <td>0.385572</td>\n",
       "      <td>0.423212</td>\n",
       "      <td>0.152932</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    1          2          3          4          5          E  \\\n",
       "test RMSE   31.959288  25.129474  31.050241  33.630573  26.963718  29.746659   \n",
       "test R2      0.233027   0.231044   0.310452   0.308028   0.346526   0.285815   \n",
       "train RMSE  29.230136  30.801063  29.481633  28.851710  30.551390  29.783186   \n",
       "train R2     0.312001   0.307153   0.290636   0.286170   0.280171   0.295226   \n",
       "bias         4.053833   4.222569   4.090705   4.029523   4.103351   4.099996   \n",
       "0           -0.185145   0.206990   0.150106  -0.287481   0.148342   0.006562   \n",
       "1           -0.535028  -0.244802  -0.281763   0.002218  -0.156190  -0.243113   \n",
       "2           -0.039962  -0.331414  -0.340516   0.029870  -0.029738  -0.142352   \n",
       "3            0.130316   0.525466   0.424846   0.214428   0.340961   0.327203   \n",
       "4            0.237178  -0.184321   0.001074  -0.095950   0.246647   0.040925   \n",
       "5            0.265198  -0.130762  -0.215418   0.198282  -0.231907  -0.022922   \n",
       "6            0.640736   0.628721   0.240620   0.565513   0.948657   0.604849   \n",
       "7            0.574431   0.301874   0.606753   0.575068   0.627206   0.537066   \n",
       "8            0.573064   0.059669   0.611451   0.858294   0.815341   0.583564   \n",
       "9           -0.457719   0.369495   0.286830   0.056105  -0.011567   0.048629   \n",
       "10           0.150073  -0.401864  -0.392294  -0.048610  -0.126313  -0.163802   \n",
       "11           0.348487   0.611964   1.130787   0.314873   0.625295   0.606281   \n",
       "12           0.967819   1.536527   0.708636   0.606355   0.747657   0.913399   \n",
       "13           0.803003   0.248467   0.026466   0.233662   0.506142   0.363548   \n",
       "14           0.112968  -0.156520  -0.001331  -0.147935  -0.033909  -0.045345   \n",
       "15           0.230173   0.186754   0.726976   0.612256   0.100950   0.371421   \n",
       "16           0.637532   0.773758   1.242153   0.277992   0.939806   0.774249   \n",
       "17           1.294922   1.080646   1.155135   0.905404   1.201230   1.127468   \n",
       "18           0.256362   0.406979   0.286445   0.153531   0.783381   0.377340   \n",
       "19           0.767448  -0.107261  -0.396583   0.417241  -0.325034   0.071162   \n",
       "20           0.152091   0.337135  -0.030420   0.178414   0.005823   0.128608   \n",
       "21           0.185779   0.766288   0.312692   0.828143   0.179224   0.454425   \n",
       "22           0.650481   0.981868   0.483993   0.425390   0.913073   0.690961   \n",
       "23           0.048967   0.600821   0.789287   0.608161   0.143100   0.438067   \n",
       "24           0.872690   0.481308   1.162291   0.947374   0.921261   0.876985   \n",
       "25          -0.147655  -0.195275  -0.050447  -0.321924  -0.188841  -0.180828   \n",
       "26           0.060411   0.955831   0.753520   0.572352   0.748701   0.618163   \n",
       "27           0.209860   0.201829   0.033375  -0.132778   0.292927   0.121043   \n",
       "28           0.411891   0.827022   0.755576   0.641470   0.756706   0.678533   \n",
       "29           2.281307   1.696922   1.345679   1.719093   1.904426   1.789485   \n",
       "30           5.792997   6.265100   5.031915   5.619893   5.585135   5.659008   \n",
       "31          -0.586966  -0.430139  -0.554167  -0.302275  -0.590787  -0.492867   \n",
       "32           1.602976   1.672224   1.559883   1.661351   1.407887   1.580864   \n",
       "33           5.690387   6.109222   5.297674   5.159223   5.417433   5.534788   \n",
       "34          -2.979768  -3.045430  -3.017600  -3.262161  -3.175854  -3.096163   \n",
       "35           0.162338   0.270715   0.176411   0.140795   0.296265   0.209305   \n",
       "36           1.447125   1.435239   2.849905   1.375271   0.981331   1.617774   \n",
       "37           0.262264   0.172443   0.121357   0.588367   0.374004   0.303687   \n",
       "38           0.771000   0.999330   1.026439   0.877061   0.797910   0.894348   \n",
       "39           0.424092   0.223100   0.548597   0.194176   0.449813   0.367956   \n",
       "40           0.192744   0.424386   0.636028   0.387287   0.709674   0.470024   \n",
       "41           0.377326   0.418348   0.804390   0.471760   0.713274   0.557020   \n",
       "42           0.866423   0.640267   1.017143   0.715932   0.885382   0.825030   \n",
       "43           0.675915   0.333509   0.817166   0.546092   0.625919   0.599720   \n",
       "44           0.479153   0.298036   0.849087   0.520984   0.691269   0.567706   \n",
       "45           0.318169   0.069147   0.606889   0.219041   0.218251   0.286300   \n",
       "46           0.418424   0.502369   0.239357   0.295634   0.224959   0.336148   \n",
       "47           0.708145   0.781429   0.452567   0.641150   0.416624   0.599983   \n",
       "48           0.484376   0.427635   0.647901   0.523983   0.187129   0.454205   \n",
       "49           0.573616   0.682171   0.474228   0.479099   0.549551   0.551733   \n",
       "50           0.513133   0.419905   0.572777   0.447975   0.112866   0.413331   \n",
       "51           0.400539   0.609058   0.640111   0.315815   0.400344   0.473173   \n",
       "52           0.207587   0.668357   0.360561   0.493982   0.385572   0.423212   \n",
       "\n",
       "                  SD  \n",
       "test RMSE   3.185687  \n",
       "test R2     0.045984  \n",
       "train RMSE  0.760349  \n",
       "train R2    0.012275  \n",
       "bias        0.066662  \n",
       "0           0.202035  \n",
       "1           0.175758  \n",
       "2           0.159900  \n",
       "3           0.141785  \n",
       "4           0.174296  \n",
       "5           0.211803  \n",
       "6           0.225509  \n",
       "7           0.119279  \n",
       "8           0.284447  \n",
       "9           0.289746  \n",
       "10          0.210748  \n",
       "11          0.292191  \n",
       "12          0.333144  \n",
       "13          0.267280  \n",
       "14          0.100016  \n",
       "15          0.249651  \n",
       "16          0.319783  \n",
       "17          0.130960  \n",
       "18          0.218511  \n",
       "19          0.449928  \n",
       "20          0.131819  \n",
       "21          0.284576  \n",
       "22          0.223142  \n",
       "23          0.288853  \n",
       "24          0.221311  \n",
       "25          0.087499  \n",
       "26          0.304160  \n",
       "27          0.152361  \n",
       "28          0.145991  \n",
       "29          0.305151  \n",
       "30          0.396367  \n",
       "31          0.111769  \n",
       "32          0.095577  \n",
       "33          0.336223  \n",
       "34          0.106018  \n",
       "35          0.062153  \n",
       "36          0.639446  \n",
       "37          0.166236  \n",
       "38          0.103231  \n",
       "39          0.136869  \n",
       "40          0.184825  \n",
       "41          0.169939  \n",
       "42          0.132893  \n",
       "43          0.159699  \n",
       "44          0.188250  \n",
       "45          0.178943  \n",
       "46          0.107491  \n",
       "47          0.142596  \n",
       "48          0.151877  \n",
       "49          0.075869  \n",
       "50          0.159315  \n",
       "51          0.127808  \n",
       "52          0.152932  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame_data = {}\n",
    "for i in range(5):\n",
    "    frame_data[str(i + 1)] = [rmse_test_arr[i], r2_test_arr[i], rmse_train_arr[i], r2_train_arr[i], bias_arr[i]]\n",
    "\n",
    "# print(data)\n",
    "\n",
    "frame_data['E'] = [np.mean(rmse_test_arr), np.mean(r2_test_arr), np.mean(rmse_train_arr), np.mean(r2_train_arr), np.mean(bias_arr)]\n",
    "frame_data['SD'] = [np.std(rmse_test_arr),np.std(r2_test_arr),np.std(rmse_train_arr),np.std(r2_train_arr), np.std(bias_arr)]\n",
    " \n",
    "df1 = pd.DataFrame(frame_data, index =['test RMSE', 'test R2','train RMSE','train R2', 'bias']) \n",
    "df2 = pd.DataFrame(\n",
    "    np.concatenate(\n",
    "        (np.hstack(coeff_arr), \n",
    "         np.mean(coeff_arr, axis=0),\n",
    "         np.std(coeff_arr, axis=0)),\n",
    "        axis=1),\n",
    "    columns=['1', '2', '3','4','5','E','SD'])\n",
    "\n",
    "df = pd.concat([df1, df2], axis=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
